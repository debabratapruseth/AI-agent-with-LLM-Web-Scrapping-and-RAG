{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8df6d5bf",
      "metadata": {
        "id": "8df6d5bf"
      },
      "outputs": [],
      "source": [
        "# üß† RAG from Website using LangChain + FAISS + OpenAI\n",
        "\n",
        "# This notebook will:\n",
        "# - Scrape a website and its internal child pages\n",
        "# - Chunk the text into small pieces\n",
        "# - Embed the chunks using OpenAI Embeddings\n",
        "# - Store them in a FAISS vector database\n",
        "# - Use Retrieval-Augmented Generation (RAG) to answer user questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6d027dc5",
      "metadata": {
        "id": "6d027dc5"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 1: Install required libraries (only run once)\n",
        "!pip install -q langchain openai faiss-cpu beautifulsoup4 tiktoken langchain-community langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d635f3a4",
      "metadata": {
        "id": "d635f3a4"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 2: Import necessary libraries\n",
        "\n",
        "import os  # For setting environment variables\n",
        "import requests  # To make HTTP requests to webpages\n",
        "from bs4 import BeautifulSoup  # To parse and extract content from HTML pages\n",
        "from urllib.parse import urljoin, urlparse  # For resolving and parsing URLs\n",
        "\n",
        "# LangChain modules\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # ‚úÖ Needed for custom LLM prompt chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain    # ‚úÖ Needed for strict prompt with context\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "# For Google Colab secrets (API key storage)\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set OpenAI API key from Colab secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4388c416",
      "metadata": {
        "id": "4388c416"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 3: Define functions to scrape the website and extract internal links\n",
        "\n",
        "def get_child_links(base_url, max_links=10):\n",
        "    \"\"\"Fetches internal links from the base URL up to a given limit.\"\"\"\n",
        "    base = urlparse(base_url)  # Parse base URL\n",
        "    domain = base.netloc  # Extract domain to compare later\n",
        "    links = set()  # Use a set to avoid duplicates\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, timeout=10)  # Send GET request to the page\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")  # Parse HTML content\n",
        "\n",
        "        for a in soup.find_all(\"a\", href=True):  # Find all <a href=...>\n",
        "            full_url = urljoin(base_url, a['href'])  # Convert relative URL to full URL\n",
        "            parsed = urlparse(full_url)\n",
        "\n",
        "            if parsed.netloc == domain and parsed.scheme.startswith(\"http\"):  # Only same-domain links\n",
        "                links.add(full_url)\n",
        "\n",
        "            if len(links) >= max_links:  # Stop if limit is reached\n",
        "                break\n",
        "\n",
        "        return list(links)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error while collecting links:\", e)\n",
        "        return []\n",
        "\n",
        "def scrape_pages(base_url, max_links=10):\n",
        "    \"\"\"Scrapes visible text and returns a list of Documents with source metadata.\"\"\"\n",
        "    all_pages = [base_url] + get_child_links(base_url, max_links)\n",
        "    documents = []\n",
        "\n",
        "    print(\"üîó Pages being scraped:\")\n",
        "    for page in all_pages:\n",
        "        print(\" -\", page)\n",
        "        try:\n",
        "            res = requests.get(page, timeout=10)\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "                tag.decompose()\n",
        "            text = soup.get_text(separator=\"\\n\").strip()\n",
        "            documents.append(Document(page_content=text, metadata={\"source\": page}))\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to read {page}: {e}\")\n",
        "\n",
        "    return documents  # A list of Document objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6d863041",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d863041",
        "outputId": "4810a423-cd25-4b07-ed9a-ca454c860842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Pages being scraped:\n",
            " - https://timesofindia.indiatimes.com/\n",
            " - https://timesofindia.indiatimes.com/city/mumbai\n",
            " - https://timesofindia.indiatimes.com/\n",
            " - https://timesofindia.indiatimes.com\n",
            " - https://timesofindia.indiatimes.com/city\n",
            " - https://timesofindia.indiatimes.com/weather\n",
            " - https://timesofindia.indiatimes.com/?loc=in\n",
            " - https://timesofindia.indiatimes.com/city/bangalore\n",
            " - https://timesofindia.indiatimes.com/city/delhi\n",
            " - https://timesofindia.indiatimes.com/city/hyderabad\n",
            " - https://timesofindia.indiatimes.com/us\n",
            "{'source': 'https://timesofindia.indiatimes.com/'}\n",
            "{'source': 'https://timesofindia.indiatimes.com/'}\n",
            "{'source': 'https://timesofindia.indiatimes.com/'}\n",
            "‚úÖ Total number of chunks created: 521\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Step 4: Scrape, chunk and embed website content\n",
        "\n",
        "# Set the URL of the website you want to analyze. We have used a newswebsite for testing\n",
        "base_url = \"https://timesofindia.indiatimes.com/\"\n",
        "\n",
        "# Step 1: Scrape the website and collect text\n",
        "raw_docs = scrape_pages(base_url, max_links=10)     # scrapts and extract data from child pages as document. We have put default to 10 child pages\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = splitter.split_documents(raw_docs)\n",
        "\n",
        "# üõ†Ô∏è DEBUG: Check metadata after splitting. Run this to check if the child page url is added as metadata in the docuements extracted. This is required to cross validate the LLM outcome is from the right child page url\n",
        "for doc in documents[:3]:\n",
        "    print(doc.metadata)\n",
        "\n",
        "# üõ†Ô∏è DEBUG: Check content extracted. Run this to check what content is ectracted per child page.\n",
        "#for doc in raw_docs[:5]:\n",
        "#    print(doc.metadata['source'])\n",
        "#    print(doc.page_content[:300])\n",
        "#    print(\"=\"*80)\n",
        "\n",
        "print(f\"‚úÖ Total number of chunks created: {len(documents)}\")\n",
        "\n",
        "# Step 3: Generate vector embeddings using OpenAI and store them in FAISS\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "\n",
        "# Save the FAISS vector index to local storage\n",
        "vectorstore.save_local(\"website_index\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a80e1281",
      "metadata": {
        "id": "a80e1281"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Step 5: Load the vector store and prepare the RAG pipeline\n",
        "\n",
        "# Load the FAISS index\n",
        "vectorstore = FAISS.load_local(\"website_index\", embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Convert vectorstore into a retriever object\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# ‚úÖ Step 1: Define strict RAG-only prompt\n",
        "# Use ChatPromptTemplate instead of PromptTemplate for better compatibility with chat models\n",
        "from langchain_core.prompts import ChatPromptTemplate # ‚úÖ Import ChatPromptTemplate\n",
        "strict_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant. Use only the context provided below to answer the question.\n",
        "If the answer is not in the context, say \"I don't know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "# ‚úÖ Step 2: Create a retrieval chain\n",
        "from langchain.chains import create_retrieval_chain # ‚úÖ Import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain # ‚úÖ Import create_stuff_documents_chain\n",
        "\n",
        "# Create a chain to combine documents\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    prompt=strict_prompt\n",
        ")\n",
        "\n",
        "# Create the retrieval chain\n",
        "qa_chain = create_retrieval_chain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=combine_docs_chain\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4d5b5958",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5b5958",
        "outputId": "cace9fcf-0ba5-48b7-eb5f-68adf5f15759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Answer:\n",
            "The latest news in Bengaluru covers a wide range of topics such as politics, crime, sports, fashion, movies, culture, dance and music, industries, civic bodies, traffic updates, weather updates, new trends, education and BBMP issues. Recently, three burglars who stole gold and other valuables from a political party functionary's house in Bengaluru were arrested. There was also an incident where a man was attacked with a machete by two youths because he refused to give his phone for a call. Moreover, experts are expressing concerns over the cut in SSLC pass mark in Karnataka, arguing that it devalues education.\n",
            "\n",
            "üìö Sources:\n",
            "- https://timesofindia.indiatimes.com/city/bangalore\n",
            "- https://timesofindia.indiatimes.com/city/bangalore\n",
            "- https://timesofindia.indiatimes.com/city/bangalore\n",
            "- https://timesofindia.indiatimes.com/city/bangalore\n"
          ]
        }
      ],
      "source": [
        "query = \"Tell me about bangalore news\"  # ask the LLM agent query. As we have used a newspaper website and have extracted only 10 child pages, we have asked a specific question here within the scope\n",
        "\n",
        "# Pass query as dict matching the prompt variable name\n",
        "result = qa_chain.invoke({\"input\": query})\n",
        "\n",
        "print(\"üß† Answer:\")\n",
        "print(result['answer'])  # ‚úÖ this works\n",
        "\n",
        "# Print source documents (from 'context' key). This is to check if the content LLM shares is from relevent child urls\n",
        "print(\"\\nüìö Sources:\")\n",
        "for doc in result['context']:\n",
        "    print(\"-\", doc.metadata.get('source', 'No metadata available'))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}